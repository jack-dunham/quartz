{"/":{"title":"ü™¥ Quartz 3.3","content":"\nHost your second brain and [digital garden](https://jzhao.xyz/posts/networked-thought) for free. Quartz features\n\n1. Extremely fast natural-language [[notes/search]]\n2. Customizable and hackable design based on [Hugo](https://gohugo.io/)\n3. Automatically generated backlinks, link previews, and local graph\n4. Built-in [[notes/CJK + Latex Support (ÊµãËØï) | CJK + Latex Support]] and [[notes/callouts | Admonition-style callouts]]\n5. Support for both Markdown Links and Wikilinks\n\nCheck out some of the [amazing gardens that community members](notes/showcase.md) have published with Quartz or read about [why I made Quartz](notes/philosophy.md) to begin with.\n\n## Get Started\n\u003e üìö Step 1: [Setup your own digital garden using Quartz](notes/setup.md)\n\nReturning user? Figure out how to [[notes/updating|update]] your existing Quartz garden.\n\nIf you prefer browsing the contents of this site through a list instead of a graph, you see a list of all [setup-related notes](/tags/setup).\n\n### Troubleshooting\n- üöß [Troubleshooting and FAQ](notes/troubleshooting.md)\n- üêõ [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n- üëÄ [Discord Community](https://discord.gg/cRFFHYye7t)\n\n\n[[notes/Notes/matrix product operator.md]]\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Literature/Dziarmaga2021":{"title":"","content":"title:: \"Time evolution of an infinite projected entangled pair state: Neighborhood tensor update\"\nauthors:: Jacek Dziarmaga\npublisher:: American Physical Society (APS)\njournal:: Physical Review B\nyear:: 2021\n\n\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Literature/Lin2021":{"title":"","content":"title:: Efficient simulation of dynamics in two-dimensional quantum spin systems with isometric tensor networks\nauthors:: Sheng-Hsuan Lin, Michael Zaletel, Frank Pollmann\npublisher:: \njournal:: 2112.08394 [cond-mat.str-el]\nyear:: 2021","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Literature/Zaletel2020":{"title":"Isometric Tensor Network States in Two Dimensions","content":"\n\n```dataview\nTABLE title AS \"Title\"\nFROM \"Literature\"\nWHERE file.name = this.file.name \n```\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Literature/bibliography":{"title":"","content":"```dataview\ntable authors, title\nfrom \"Literature\"\nsort authors asc\n```\n\n\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Notes/isometric-tensor-network":{"title":"","content":"\nAn **isometric tensor network state (isoTNS)** is a generalisation of the canonical form of a [[matrix product state]] (MPS) to higher dimensions, with the fundamental difference that these higher dimensional generalisations are *approximate*---while every MPS can be gauged into an isometric form, that is not the case in general for a [[project entangled pair state]] (PEPS). As such, the set of isometric PEPS (isoPEPS) are a subset of PEPS, and thus are less expressible ansatze. The isoTNS ansatz was first introduced in Ref. [[@Zaletel2020]] along with a 2D version of the [[time-evolving block decimation]] algorithm (TEBD$^2$) applied to the [[transverse field ising model]]. In a subsequent paper, a 2D version of the [[density matrix renormalisation group]] algorithm (DMRG$^{2}$) was introduced with an additional application to the Kitaev model on a honeycomb lattice [[@Lin2021]].   \n## Variational Moses move\n## Sequential Moses move\n\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Notes/matrix-product-operator":{"title":"","content":"\nIn analogy to that of the [[matrix product state]] (MPS) representation of quantum states, operators can also be written as products of matrices. Consider the operator $O$ given by\n\n$$\n    O = \\sum_{\\{s_i\\}, \\{s'_i\\} } O^{s_1, \\ldots, s_N}_{s'_1, \\ldots, s'_N}\\ketbra{s'_1\\cdots s'_N}{s_1,\\cdots,s_N},\n$$\n\nor graphically:\n```tikz\n\\begin{document}\n\t\\begin{tikzpicture}[t/.style = {draw, thick, minimum size=0.8cm}, thick, scale=1.2]\n\t        \\node (0) at (0.3,0) {};\n\t        \\node (5) at (4.7,0) {};\n\t        \\node[t, label=center:$O^{[1]}$] (1) at (1,0){};\n\t        \\node[t, label=center:$O^{[2]}$] (2) at (2,0){};\n\t        \\node (3) at (3,0){$\\ldots$};\n\t        \\node[t, label=center:$O^{[N]}$] (4) at (4,0){};\n\t        \\foreach \\i in {0,...,4}\n\t        {\n\t          \\pgfmathtruncatemacro{\\iplus}{\\i + 1};\n\t          \\draw[-] (\\i) -- (\\iplus);\n\t        }\n\t        \\foreach \\i in {1,2,4}\n\t        {\n\t          \\draw[-] (\\i) -- (\\i, -0.6);\n\t          \\draw[-] (\\i) -- (\\i, 0.6);\n\t        }\n\t\\end{tikzpicture}\n\\end{document}\n```\nNow suppose the operator can be expressed as\n\n$$\n    O = \\sum_{i=1}^{N-1} A_i A_{i+1} + \\sum_{i=1}^{N} B_i\n$$\n\nThe corresponding tensors $M^{[i]s_i's_i}_{ \\alpha_{i-1} \\alpha_{i} }$ in the MPO representation are then\n\n$$\n    M^{ [i] s_i' s_i}_{\\alpha_{i-1 } \\alpha_{i}} = \\begin{pmatrix}\n        \\mathbb{I}^{s_i' s_i} \u0026 A_i^{s_i' s_i} \u0026 B_i^{s_i' s_i} \\\\\\\\\n        0 \u0026 0 \u0026 A_i^{s_i' s_i} \\\\\\\\\n        0 \u0026 0 \u0026 \\mathbb{I}^{s_i' s_i}\n    \\end{pmatrix}\n$$\n\nsuch that\n\n$$\n    O = \\sum_{\\\\{s_i\\\\},\\\\{s_i'\\\\} }\n    M^{[1]s_1's_{1} }_{1 \\alpha_{1} }\n    \\cdots\n    M^{[i]s_i's_i}_{\\alpha_{i-1} \\alpha_{i}  }\n    \\cdots\n    M^{[N]s_{N}'s_{N} }_{\\alpha_{N} 1}\n    \\ketbra{s'_1\\cdots s'_N}{s_1,\\cdots,s_N}\n$$\n\n\nThe left- and right-most matrices $M^{[1]s_1's_{1} }_{1 \\alpha_{1} }$ and $M^{[N]s_N's_{N} }_{ \\alpha_{N-1} 1}$ are given by\n\n$$\n    M^{[1]s_1's_{1} }_{1 \\alpha_{1} } = \\begin{pmatrix}\n        \\mathbb{I}^{s_1's_1} \u0026 A_1^{s_1's_1} \u0026 B_1^{s_1's_1}\n    \\end{pmatrix}\n    \\qq{and}\n    M^{[N]s_N's_{N}}_{\\alpha_{N-1}1} = \\begin{pmatrix}\n        \\mathbb{I}^{s_N's_N} \\\\\\ A_N^{s_N's_N} \\\\\\ B_N^{s_N's_N}\n    \\end{pmatrix}\n$$\n\n\n## Further reading\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Notes/matrix-product-state":{"title":"","content":"\u003e cont. from written notes\n\nRepeat SVD until one open index per tensor. \n\nThe wavefunction in this form is known as a *matrix product state. \n\nA matrix product state looks like this:\n```tikz\n\\begin{document}\n\t\\begin{tikzpicture}\n\t\t\\foreach \\x in {1,...,5}{\n\t\t\\node[draw, minimum size=0.5cm, thick] (\\x) at (\\x,0) {};\n\t\t\\node (s\\x) at (\\x, -1) {$s_\\x$};\n\t\t\\draw[thick, red] (\\x) to (s\\x);\n\t\t}\n\t\t\\foreach \\x in {1,...,4}{\n\t\t\\pgfmathtruncatemacro{\\xp}{\\x + 1};\n\t\t\\draw[thick] (\\x) to (\\xp);\n\t\t}\n\t\\end{tikzpicture}\n\\end{document}\n```\nA matrix product operator looks like this:\n```tikz\n\\begin{document}\n\t\\begin{tikzpicture}\n\t\t\\foreach \\x in {1,...,5}{\n\t\t\\node[draw, minimum size=0.5cm, thick] (\\x) at (\\x,0) {};\n\t\t\\node (s\\x) at (\\x, -1) {$s_\\x$};\n\t\t\\node (t\\x) at (\\x, 1) {$t_\\x$};\n\t\t\\draw[thick, red] (\\x) to (s\\x);\n\t\t\\draw[thick, red] (\\x) to (t\\x);\n\t\t}\n\t\t\\foreach \\x in {1,...,4}{\n\t\t\\pgfmathtruncatemacro{\\xp}{\\x + 1};\n\t\t\\draw[thick] (\\x) to (\\xp);\n\t\t}\n\t\\end{tikzpicture}\n\\end{document}\n```\n\n\n\n\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Notes/representation-theory":{"title":"","content":"A $n$-dimensional representation of a group $G$ on a vector space $\\mathbb{V}$ denoted $\\Gamma$ is a mapping from $G$ to the set $GL(\\mathbb{V})$. The $n$-dimensional vector space $\\mathbb{V}$ is known as the **representation space** of $\\Gamma$. \n\n## Irreducible representations\nThe representation space $\\mathbb{V}$ can, in general, be decomposed into a set of **invariant subspaces** $\\mathbb{V}_l$ each with dimension $d_{i}\u003c n$. That is,\n$$\n\\mathbb{V} \\cong \\mathbb{V}_{1}\\oplus \\mathbb{V}_{2}\\oplus\\mathbb{V}_{3}\\oplus \\ldots\n$$\nA vector $\\mathbf{v}\\in \\mathbb{V}_l$ remains in $\\mathbb{V}_i$ upon the action of an element of the group $G$:\n$$\n\\mathbf{v}\\in \\mathbb{V}_l \\, \\Rightarrow \\,\\Gamma(g)\\mathbf{v} \\in \\mathbb{V}_l, \\quad \\forall g \\in G,\\mathbf{v} \\in \\mathbb{V}_l\n$$\nEach invariant subspace corresponds to an [[irreducible representation]] (irrep) of the group $G$. What this means is that you can write any representation $\\Gamma(g)$ in block diagonal form, where the blocks are irreducible representations of $G$ denoted $U_l(g)$\n$$\n\\Gamma(g) \\cong¬†\n\n\\begin{pmatrix} U_1(g)\u0026\u0026\\\\\u0026U_2(g)\u0026\\\\\u0026\u0026U_3(g) \\\\ \\end{pmatrix} \n$$\nThe irrep $U_{l}(g)$ corresponds to the invariant subspace $\\mathbb{V}_{l}$. An irreducible representation cannot be reduced further. That is, an irreducible representation cannot be block-diagonalised into smaller representations still.  For an **abelian group**, all the irreps are one-dimensional.","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Notes/symmetric-quantum-systems":{"title":"","content":"## Abelian groups\nSuppose a hamiltonian $H$ is symmetric with respect to an abelian group $\\mathcal{G}$ with group elements $g \\in \\mathcal{G}$. That is, suppose\n$$\nU(g) H \\,U^{\\dagger}(g) = H \\quad \\forall g \\in \\mathcal{G}\n,$$\nwhere $U(g)$ is a unitary representation of $g \\in \\mathcal{G}$. Then the Hilbert space of $H$ decomposes into a direct sum of invariant subspaces of the group $\\mathcal{G}$. That is, from the perspective of $\\mathcal{H}$ being the representation space of the representation $U$, we can write $\\mathcal{H}$ as a direct sum of invariant subspaces.  \n\nFor $\\ket{\\psi_{l}} \\in \\mathbb{V}^{l}$, the group generator $j \\in \\mathcal{B}(\\mathcal{H})$ is the operator such that\n$$\nj\\ket{\\psi_{l}} = l\\ket{\\psi_{l}}\n$$","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Projects/isometric-locally-purified-tensor-network":{"title":"","content":"\n## Background and motivation\nThis is effectively combining an [[isometric tensor network]] ansatz, with a locally purified one, motivated by the following:\n1. It is potentially efficient in truncation by making use of an isometric tensor network ansatz (isoTNS)---no approximate contractions required.\n2. Potentially efficient in calculation of observables. To see this, note that the isometric form makes calculating quantities like $\\tr(\\rho^{\\dagger} \\mathcal{O}_{i}\\rho)$, with local $\\mathcal{O}_i$ trivial, however for mixed states, $\\braket{\\mathcal{O}_{i}}= \\tr(\\mathcal{O}_{i} \\rho )$ and thus the isometric structure cannot be exploited. If instead we represent $X$, where $\\rho = X X^{\\dagger}$,  as a isometric tensor network, then observables can be calculated efficiently. \n\t- This has the additional benefit that the ansatz remains positive.\n3. One could then potentially use a time-evolution algorithm to compute approximations to eigenvalues of the Liouvillian spectrum via so-called Arnoldi-Lindblad evolution. \n\nSpecifically in the context of the work in this group:\n1. We wish to investigate finite systems *without* translational invariance, such as strained graphene. \n2. We can do finite scaling analysis to verify the output of iPEPO based algorithms. \n\n## Potential problems\nThere is no theoretical basis suggesting isometric tensor networks can represent steady states of the Liouvillians of interest (or any). Work would be appreciated here. Potentially possible to prove within the context of quantum circuits as quantum circuits are naturally isometric. Perhaps it would be wise to get in contact with Andrew Green about this. This is likely a hard problem however, as very little progress has been made *at all* in the theory connecting TNs and open quantum systems. I am unsure whether this is because it is very hard (likely) or that closed systems are yielding enough results that researchers haven't bothered yet (also likely). I reckon that making connections between isoTNS and steady states of certain Liouvillians is an easier problem than connecting the latter with quantum information theory such as proving area-law entanglment growth. Probably a bit too ambitious. \n\nIsometric tensor networks require a \"Moses move\"  step to move the orthogonality centre. This is approximate, and will introduce error proportional to the system size and number of time steps. This is unlikely a problem provided the system converges to the steady state fast enough, as such, in contrast to coherent time-evolution, minimising the error at each time step is may be more important than minimising total compounding error. On the other hand, choosing a too small time step may result in the error dominating the time evolution resulting in poor convergence. \n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null},"/notes/Tensor-School-2022":{"title":"","content":"## Tensor products of matrices\nThe Kronecker product of an $n_1 \\times n_2$ matrix $A$ and a $m_1 \\times m_2$ matrix $B$ is usually defined as the following $n_1 m_1 \\times n_2 m_2$ matrix:\n$$\n\tA \\otimes B = \n\t\t\\begin{pmatrix}\n\t\t\ta_{11}B \u0026 \\cdots \u0026 a_{1 n_2} B \\\\\n\t\t\t\\vdots \u0026 \\ddots  \u0026 \\vdots \\\\\n\t\t\ta_{n_1 1} B \u0026 \\cdots \u0026 a_{n_1 n_2} B\n\t\t\\end{pmatrix}\n$$\n\u003e [!NOTE] The above is the most commonly used convention, but not the only one that can be chosen.\n\nThe Kronocker product has the following properties:\n- It is bilinear, i.e. linear in both arguments.\n- It is associative, that is $(A\\otimes B) \\otimes C  =  A \\otimes (B \\otimes C)$\n- It is generally *non*-commutative, *but* $B \\otimes A = P (A \\otimes B) Q^{-1}$ for some permutations $P$ and $Q$\n- $(A\\otimes B)(C \\otimes D) = AC\\otimes BD$\n### Pauli matrices\nThe Pauli matrices are ubiquitous throughout physics. We use the following definitions\n$$\n\\sigma^x = \\mqty(\\pmat{1}), \\quad \\sigma^y = \\mqty(\\pmat{2}) \\qq{and} \\sigma^z = \\mqty(\\pmat{3}) \n$$\nwith a basis $\\ket{\\uparrow}  = \\smqty(1 \\\\ 0)$ and $\\ket{\\downarrow} = \\smqty(0 \\\\ 1)$.  We can also define the Pauli raising and lowering operators as\n$$\n\\sigma^+ = \\mqty(0 \u0026 1 \\\\ 0 \u0026 0) \\qq{and} \\sigma^- = \\mqty(0 \u0026 0 \\\\ 1 \u0026 0) \n$$\nrespectively.\n\n## Heisenberg model with $S=1/2$\nConsider the following Hamiltonian $H$ corresponding to the Heisenburg model for spin-half particles:\n$$\nH = \\sum_i \\left( S_i^x S_{i+1}^x + S_i^y S_{i+1}^y + S_i^z S_{i+1}^z\\right) + h \\sum_i S_i^z\n$$\nwith spin matrices $S^{\\alpha} = \\frac{1}{2} \\sigma^{\\alpha}$. How can we construct $H$ explictly? Consider the case of only two sites $H^{(2)}$\n$$\nH^{(2)} = S_i^x S_{i+1}^x + S_i^y S_{i+1}^y + S_i^z S_{i+1}^z+ h (S_i^z \\otimes \\mathbb{I} + \\mathbb{I} \\otimes S_i^z).\n$$\nNow if we define $S^{\\alpha (2)} = \\mathbb{I} \\otimes S^{\\alpha}$ and $\\mathbb{I}^{(2)} = \\mathbb{I} \\otimes \\mathbb{I}$, then the three site Hamiltonian can be written as \n$$\nH^{(3)} = H^{(2)} \\otimes \\mathbb{I} + S^{x (2)} \\otimes S^x + S^{y (2)} \\otimes S^y + S^{z (2)} \\otimes S^z + h(\\mathbb{I}^{(2)} \\otimes \\mathbb{I}).\n$$\nFollowing the pattern, we can define a recurrence relation for a general $n$-site Hamiltonian. Let\n$$\nS^{\\alpha (n + 1)} = \\mathbb{I}^{(n)} \\otimes S^{\\alpha} \\qq{and} \\mathbb{I}^{(n+1)} = \\mathbb{I}^{(n)} \\otimes \\mathbb{I}\n$$\nthen \n$$\nH^{(n+1)} = H^{(n)} \\otimes \\mathbb{I} + S^{x (n)} \\otimes S^x + S^{y (n)} \\otimes S^y + S^{z (n)} \\otimes S^z + h(\\mathbb{I}^{(n)} \\otimes \\mathbb{I}).\n$$\nThe size of the matrix $H^{(2)}$ is $2^{n} \\times 2^n$, i.e. exponential in the number of particles $n$. Pratically, this presents a challenge as modern supercomputers can only store \"small\" matrices, and there is no concievable way to store the exact Hamiltonian of a $100$ particle system, as this would require more matter than exists in the Universe. A proposed solution is to *make $H^{(n)}$ smaller* by truncating the Hilbert space onto *relevant* degrees of freedom. As an initial guess, one might chose to project onto the $m$ lowest eigenstates of $H^{(n)}$. We can capitulate this into the following following algorithm.\n\nThis algorithm is akin to the numerical renormalization group (NRG), with a slight difference which I cannot remember, but that doesnt matter---the point is that this truncation procedure performs *poorly*. We might that be? Fundamentally we are constructing eigenfunctions of a $(n+1)$-site lattice from eigenfunctions of a $n$-site lattice. Looking at the form of these eigenfunctions, which are essentially non-iteracting and thus plane waves, we can say that issues arrise at the related to the boundary conditions. \n\n## Steven White (1992)\nInstead of projecting onto the eigenstates of a smaller system, we instead project the *wavefunction* onto a subset of the *reduced density matrix*. \n\n\n","lastmodified":"2022-09-21T16:56:28.846483485Z","tags":null}}